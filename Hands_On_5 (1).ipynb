{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfVybTQYMQb",
        "outputId": "83fbb52b-608a-46b9-8d9b-59ee44030e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 300 Posts:\n",
            "                                               title  \\\n",
            "0  Harris win would be a 'big relief' for climate...   \n",
            "1  Billions of crabs went missing around Alaska. ...   \n",
            "2  We’re Not Prepared for What Hurricane Milton M...   \n",
            "3  $266 Trillion in Climate Spending Is a No-Brai...   \n",
            "4  Trump would be an \"Extinction-Level Event\" for...   \n",
            "\n",
            "                                                 url      subreddit  \\\n",
            "0          https://www.dailymotion.com/video/x976tya  climatechange   \n",
            "1  https://www.yahoo.com/news/billions-crabs-went...  climatechange   \n",
            "2  https://slate.com/technology/2024/10/hurricane...  climatechange   \n",
            "3  https://www.bloomberg.com/opinion/articles/202...  climatechange   \n",
            "4  https://www.juancole.com/2024/11/extinction-tu...  climatechange   \n",
            "\n",
            "   num_comments   created_utc  \n",
            "0            81  1.729274e+09  \n",
            "1           739  1.697764e+09  \n",
            "2           623  1.728321e+09  \n",
            "3           678  1.699974e+09  \n",
            "4           829  1.730820e+09  \n",
            "\n",
            "Climate Change Posts with More Than 100 Comments:\n",
            "                                               title  \\\n",
            "1  Billions of crabs went missing around Alaska. ...   \n",
            "2  We’re Not Prepared for What Hurricane Milton M...   \n",
            "3  $266 Trillion in Climate Spending Is a No-Brai...   \n",
            "4  Trump would be an \"Extinction-Level Event\" for...   \n",
            "6  Floridians are getting the hint , climate chan...   \n",
            "\n",
            "                                                 url      subreddit  \\\n",
            "1  https://www.yahoo.com/news/billions-crabs-went...  climatechange   \n",
            "2  https://slate.com/technology/2024/10/hurricane...  climatechange   \n",
            "3  https://www.bloomberg.com/opinion/articles/202...  climatechange   \n",
            "4  https://www.juancole.com/2024/11/extinction-tu...  climatechange   \n",
            "6  https://www.reddit.com/r/climatechange/comment...  climatechange   \n",
            "\n",
            "   num_comments   created_utc  \n",
            "1           739  1.697764e+09  \n",
            "2           623  1.728321e+09  \n",
            "3           678  1.699974e+09  \n",
            "4           829  1.730820e+09  \n",
            "6           876  1.723357e+09  \n",
            "\n",
            "Top 300 Posts with Comment Categories:\n",
            "                                               title  num_comments  \\\n",
            "0  Harris win would be a 'big relief' for climate...            81   \n",
            "1  Billions of crabs went missing around Alaska. ...           739   \n",
            "2  We’re Not Prepared for What Hurricane Milton M...           623   \n",
            "3  $266 Trillion in Climate Spending Is a No-Brai...           678   \n",
            "4  Trump would be an \"Extinction-Level Event\" for...           829   \n",
            "\n",
            "       comment_category  \n",
            "0  medium comment posts  \n",
            "1    high comment posts  \n",
            "2    high comment posts  \n",
            "3    high comment posts  \n",
            "4    high comment posts  \n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Initialize PRAW with your Reddit application credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id='BmGfpxrgpA4U8yJZNl3Y2g',\n",
        "    client_secret='hAzVgiZAFzsIeLAAfE_yga9VQtElyg',\n",
        "    user_agent='My Reddit Data Scraper'\n",
        ")\n",
        "\n",
        "\n",
        "# Function to scrape posts from a subreddit\n",
        "def scrape_subreddit(subreddit_name):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts = []\n",
        "    for post in subreddit.top(limit=100):\n",
        "        posts.append({\n",
        "            'title': post.title,\n",
        "            'url': post.url,\n",
        "            'subreddit': subreddit_name,\n",
        "            'num_comments': post.num_comments,\n",
        "            'created_utc': post.created_utc\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "# Scrape the top posts\n",
        "climatechange_posts = scrape_subreddit('climatechange')\n",
        "climate_posts = scrape_subreddit('climate')\n",
        "environment_posts = scrape_subreddit('environment')\n",
        "\n",
        "# Combine posts into a single DataFrame\n",
        "top_300_posts = pd.DataFrame(climatechange_posts + climate_posts + environment_posts)\n",
        "\n",
        "# Remove duplicates\n",
        "top_300_posts.drop_duplicates(subset=['title'], inplace=True)\n",
        "\n",
        "# Save to CSV\n",
        "top_300_posts.to_csv('top_300_posts.csv', index=False)\n",
        "\n",
        "# Display the results of the top 300 posts\n",
        "print(\"Top 300 Posts:\")\n",
        "print(top_300_posts.head())  # Display first few rows\n",
        "\n",
        "# Filter posts with over 100 comments in 'climatechange'\n",
        "cc_highcomments = top_300_posts[(top_300_posts['subreddit'] == 'climatechange') &\n",
        "                                  (top_300_posts['num_comments'] > 100)]\n",
        "\n",
        "# Save to CSV\n",
        "cc_highcomments.to_csv('cc_highcomments.csv', index=False)\n",
        "\n",
        "# Display the results of high comment posts\n",
        "print(\"\\nClimate Change Posts with More Than 100 Comments:\")\n",
        "print(cc_highcomments.head())  # Display first few rows\n",
        "\n",
        "# Create a new variable for comment categories\n",
        "def categorize_comments(num_comments):\n",
        "    if num_comments < 50:\n",
        "        return 'low comment posts'\n",
        "    elif 51 <= num_comments <= 100:\n",
        "        return 'medium comment posts'\n",
        "    else:\n",
        "        return 'high comment posts'\n",
        "\n",
        "# Apply the categorization function\n",
        "top_300_posts['comment_category'] = top_300_posts['num_comments'].apply(categorize_comments)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV\n",
        "top_300_posts.to_csv('top_300_posts_with_categories.csv', index=False)\n",
        "\n",
        "# Display the results with categories\n",
        "print(\"\\nTop 300 Posts with Comment Categories:\")\n",
        "print(top_300_posts[['title', 'num_comments', 'comment_category']].head())  # Display first few rows\n",
        "\n"
      ]
    }
  ]
}